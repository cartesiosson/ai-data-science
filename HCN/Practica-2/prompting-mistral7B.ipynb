{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6602da5-3c8d-40dd-9211-a0009357d69a",
   "metadata": {},
   "source": [
    "# Experimenting Prompt Engineering - Chatbot\n",
    "\n",
    "In this notebook we will explore common prompt engineering techniques to get the most out of Large Language Models (LLMs). \n",
    "\n",
    "First you will experiment by loading a 7 billion parameter LLM within the notebook environment and throwing some prompts its way to see how differt prompt strategies can impact the results of the model. \n",
    "\n",
    "After trying different prompt engineering techniques, you will learn how to run a simple chatbot using what you learned. \n",
    "\n",
    "Finally, we'll review some pointers for how to put these concepts into practice by building on the sample code and swapping in different LLMs to create your own solutions.\n",
    "\n",
    "This notebook is used at workshop named AIM219 at re:Invent. Please refer the presentation slide from here: [Learn and experiment with LLMs in Amazon SageMaker Studio Lab (AIM219)](https://speakerdeck.com/icoxfog417/learn-and-experiment-with-llms-in-amazon-sagemaker-studio-lab-aim219). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091fdae7-bb30-4e60-a2ec-01eaf20657e2",
   "metadata": {},
   "source": [
    "### Working Environment \n",
    "\n",
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/generative-ai/mistral/prompting-mistral7B.ipynb)\n",
    "\n",
    "\n",
    "This notebook has been designed, written and tested to run for free on [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/) with CPU.  Studio Lab is a free machine learning (ML) development environment that provides compute and storage (up to 15GB) at no cost with NO credit card required.\n",
    "\n",
    "You can sign up for Amazon SageMaker Studio Lab here: https://studiolab.sagemaker.aws/\n",
    "\n",
    "> Whatever environment you end up using, make sure you have at least 12 GB of disk space available to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8a19a-8522-4bcf-b2a5-54cfcad0aead",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "First, if needed, install `ctransformers` - Python bindings implemented in C/C++ to accelerate the inference speed of models built on [`transformers` library](https://github.com/huggingface/transformers). If you run this notebook locally or on a GPU instance (ex. g4 or g5) instead of Studio Lab, please check that appropriate version of the CUDA runtime (`nvcc`) is installed. If you face a \"command not found\" error when running `nvcc`, [installing cuda-toolkit](https://anaconda.org/nvidia/cuda-toolkit) after checking which CUDA version is needed via `nvidia-smi` should fix the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa1ac45-da8a-441e-83d6-65e8132bba2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in ./.venv/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (0.34.2)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from transformers[torch]) (2.4.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate>=0.21.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (75.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.venv/lib/python3.12/site-packages (from torch->transformers[torch]) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.6.68)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ctransformers in ./.venv/lib/python3.12/site-packages (0.2.27)\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.12/site-packages (from ctransformers) (0.25.0)\n",
      "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in ./.venv/lib/python3.12/site-packages (from ctransformers) (9.0.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub->ctransformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub->ctransformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub->ctransformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub->ctransformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub->ctransformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub->ctransformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub->ctransformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub->ctransformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub->ctransformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub->ctransformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub->ctransformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.12/site-packages (0.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.12/site-packages (from langchain) (0.1.125)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in ./.venv/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]\n",
    "%pip install ctransformers\n",
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dc104e-d8e9-489b-ab21-dbcecbc3ecca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b4c963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.12/site-packages (0.25.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9f8bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04e3ec-9978-45f5-9a00-e41ba50804f4",
   "metadata": {},
   "source": [
    "## Mistral-7B-Instruct-v0.1-GGUP Model\n",
    "\n",
    "The 🐋 [Mistral-7B-Instruct-v0.1-GGUP](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) 🐋 model was fine-tuned on top of Mistral 7B using using a variety of publicly available conversation datasets. \n",
    "\n",
    "### Loading the model\n",
    "\n",
    "The following cell will load the pretrained model. Because the model itself is around 3.8 GB, so it can take a minute or so to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d237a5b-0ee3-4b48-a426-4048b186a375",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548a940d925b4a8f8e93360cfc38d482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bfcd77b8b74145aacd0521992a3b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    model_type=\"mistral\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a31903-630d-4b12-bea0-a77a872b61ac",
   "metadata": {},
   "source": [
    "# How do large language models work?\n",
    "\n",
    "## Prompt engineering\n",
    "\n",
    "Remember that LLMs are trained to predict the next word when given a sequence of words. \n",
    "\n",
    "![LLM-concept](llm-generation.png)\n",
    "\n",
    "The way that we prompt an LLM can make a big difference in how it responds and how useful its output is. This depends not only on they type of query, but also nuances of how individual models where trained. Let's start by asking the Mistral model to describe the concept of photosynthesis.\n",
    "\n",
    "## Let's try a basic prompt\n",
    "\n",
    "In the following cell, we provide the model three parameters:\n",
    "1. the prompt: \"Explain photosythesis\"\n",
    "2. max_new_tokens which is the maximum length in tokens for the model's response\n",
    "3. temperature which you can think of as a measure of how *creative* the model can be in its response\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> If you are running this on a cpu instance of Studio Lab keep in mind that the following cell may take a couple minutes to run.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5a91d1-2198-4583-90b0-8841885b3e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose (sugar), oxygen, and other chemical compounds. It is an essential process for life on Earth because it provides the primary source of organic compounds and oxygen.\n",
      "\n",
      "The process of photosynthesis occurs in two stages: the light-dependent reactions and the light-independent reactions. During the light-dependent reactions, energy from sunlight is absorbed by pigments such as chlorophyll in the thylakoid membranes of the chloroplasts. This energy is used to generate ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate), which are high-energy molecules that are used in the second stage of photosynthesis.\n",
      "\n",
      "During the light-independent reactions, also known as the Calvin cycle, ATP and NADPH are used to power a series of chemical reactions that convert carbon dioxide into glucose. This process takes place in the stroma of the ch"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain photosythesis\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79335c8f-ede7-4a65-813e-ec225822d5db",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model did pretty well. Notice it started with generating a period (\".\"). That's because LLMs are trained to generate next most probably token, given an input. Let's see if we can make the answer more concise, and we will end our prompt with a period this time.</b></div>\n",
    "\n",
    "## Modifying the basic prompt\n",
    "Below we will try giving the model more specific instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18fe60fe-bc65-4855-819e-a7ecb3071e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose, oxygen, and other chemical compounds. It is an essential process for life on Earth as it provides the primary source of energy and food for almost all living organisms. The process occurs in two stages: the light-dependent reactions, which take place in the thylakoid membranes of chloroplasts, and the light-independent reactions, also known as the Calvin cycle, which occur in the stroma of chloroplasts."
     ]
    }
   ],
   "source": [
    "prompt = \"Explain photosythesis in three sentences.\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7071987-b179-4cd2-900c-28e02894c7d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>This response is more concise, our prompt worked quite well.</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc398e-65a6-4ed6-99bc-19a3f2fa0498",
   "metadata": {},
   "source": [
    "# What is Fine tuning?\n",
    "\n",
    "When interacting with large lanauge models, you can enhance the model performance through fine tuning.\n",
    "\n",
    "Instruction tuning is a type of fine-tuning. We should be careful with the term \"fine-tuning\" because it encompasses many types of training in large language model context. For example, continued pre-training is another type of fine-tuning that aims to add additional knowledge beyond the base model. Training on domain-specific corpora such as medical articles, social media texts, and specific language data are examples. In contrast, instruction tuning aims to refine model behavior rather than adding knowledge. As a result of instruction tuning, a model can produce accurate responses with short or formatted prompts. It just like adding call-and-response style to the model.\n",
    "\n",
    "Recent research enables \"switching\" model style by switching additional model weights to base model. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) is one way to freeze original model weights and fine-tune with additional weights. For examples, you can generate artworks in different styles by switching LoRA model weights of Stable Diffusion. We can find various weights online such as [CivitAI](https://civitai.com/). (However we should be mindful of copyright - in most cases, emulating specific author's style and damaging their repuration or opportunities is forbidden by law.)\n",
    "\n",
    "\"Fine tuning\", which includes both instruction tuning and continued pre-training, requires datasets, computing resources, and extensive training time and effort. We have confirmed that we can change the behavior by chainging only the prompt. This is called \"Prompt tuning\". We need to consider the most appropriate ways to elicit the desired responses from large language models. The course \"[Finetuning Large Language Models](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)\" from DeepLearning.AI provides helpful guide for this decision.\n",
    "\n",
    "# Instruction fine-tuning\n",
    "\n",
    "Luckily, `Mistral-7B-Instruct` is already instruction tuned, meaning we could instruct the model to do specific tasks. This particular model was instruction tuned as described in the [model card](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). It appears the model uses the following markup language:\n",
    "```\n",
    "<s>[INST] {prompt} [/INST]\n",
    "```\n",
    "\n",
    "Here is an example prompt exchange using the model template:\n",
    "\n",
    "```\n",
    "<s>[INST] What is your favourite condiment? [/INST]\n",
    "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\n",
    "[INST] Do you have mayonnaise recipes? [/INST]\n",
    "```\n",
    "\n",
    "## Let's try it:\n",
    "\n",
    "The following cell uses the prompt template to provide a system prompt to the model to ask it to provide its reasoning step-by-step which can help LLM response accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df8390b3-2d2b-4bfb-ab8c-223dc5455321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Please tell me about how mistral winds have attracted super-orcas. Keep your answer brief. [/INST]\n",
      "\n",
      "Response:\n",
      "\n",
      "Mistral winds are strong, cold winds that blow across the Mediterranean Sea from France to Italy. These winds have been known to attract super-orcas, also known as killer whales, due to their nutrient-rich waters and favorable conditions for hunting prey. The mistral winds create upwelling, which brings nutrients from the deep ocean to the surface, providing a plentiful food source for the orcas. Additionally, the strong currents created by the mistral winds help to concentrate schools of fish, making it easier for the orcas to hunt. Overall, the combination of nutrient-rich waters and favorable hunting conditions has made the Mediterranean Sea an attractive destination for super-orcas, drawing them from as far away as Norway and Iceland."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] Please tell me about how mistral winds have attracted super-orcas. Keep your answer brief. [/INST]\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    " for text in llm(prompt, max_new_tokens=500, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "     print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b7a08-1c97-47ef-a659-7cf30315567d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model was able to follow the system direction to provide its step-by-step reasoning. However, manually supplying the full prompt template for each query is cumberson. In the next section we will look at how to simplify and automate prompt generation dynamically.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046f28b-a89f-41f5-9d54-89263a396039",
   "metadata": {},
   "source": [
    "# Dyanamic prompting\n",
    "Now that we know how to submit instructions to Mistral LLM, let's try assembling prompts dynamically - part of the prompt can be fixed, and part can be provided on the fly. We achieve this by creating prompt template with variables. The value for each variable is supplied in a separate statement and that statement can be executed further down in the code. Basically, this is a way to de-couple static part of the prompt from variable one, making the entire prompt dynamic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa310f2-356c-47aa-9679-cb7cba07ed48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "<s>[INST] {question} [/INST]    \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59b97e69-cfaf-4d51-973c-797629f2c86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Explain photosynthesis in three sentences [/INST]    \n",
      "\n",
      "Response:\n",
      "Photosynthesis is the process by which plants, algae and some bacteria convert sunlight, water and carbon dioxide into glucose, oxygen and other chemical compounds. It is an essential process for the survival of most life forms on Earth, as it provides the primary source of energy and food for nearly all living organisms. The process occurs in two stages: the light-dependent reactions, which take place in the thylakoid membranes of chloroplasts and involve the absorption of light energy by pigments such as chlorophyll, and the light-independent reactions, also known as the Calvin cycle, which occur in the stroma of chloroplasts and involve the fixation of carbon dioxide into glucose through a series of chemical reactions."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=\"Explain photosynthesis in three sentences\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=500, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8c068-56e8-46a1-b2e2-f4c01e115c95",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>Using prompt templates makes it much easier to automate away the formatting nuances of different LLMs which can vary widely. Also notice, we did not finish our sentence with a period and the model did not try to complete it, becuase this time we signaled it that this is a command, an instrution which needs to be followed.</b></div>\n",
    "\n",
    "\n",
    "# Few-shot prompts\n",
    "\n",
    "We can influence model's output style by using what's called few-shot prompting - a technique which shows the model the exact behavior we expect on few examples. Few-shot prompting can be a powerful approach to improve model performance for a given task without additional training or fine-tuning. \n",
    "\n",
    "As you can see in the cell below, we provide the model two examples of a prompt and the expected response where the number of response sentences is constrained correctly by the request. We also influence the output formatting by demonstrating that we prefer sentences to be number in each response. \n",
    "\n",
    "In the end the full prompt with our request for a three sentence explanation of photosenthesis and let the model complete the example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1e47308-970e-4211-9923-26eed91d2809",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose (sugar), oxygen, and other chemical compounds.\n",
      "2. It is an essential process for the survival of most life forms on Earth, as it provides the primary source of energy and organic compounds for almost all living organisms.\n",
      "3. Photosynthesis occurs in two stages: the light-dependent reactions, which take place in the thylakoid membranes of chloroplasts, and the light-independent reactions, also known as the Calvin cycle, which occur in the stroma of chloroplasts."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<s>[INST] Explain precipitation in two sentences [/INST]\n",
    "1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\n",
    "2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.</s>\n",
    "[INST] Explain condensation in one sentence [/INST]\n",
    "1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.</s>\n",
    "[INST] Explain photosynthesis in three sentences  [/INST]\n",
    "\"\"\"\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02938f76-69c4-408c-9a59-7200293814e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>And now with langchain template:</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a23538-bed3-46c5-9343-16aaf9a86f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<s>[INST] Explain precipitation in two sentences. [/INST]\n",
      "1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\n",
      "2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.</s>\n",
      "\n",
      "<s>[INST] Explain condensation in one sentence. [/INST]\n",
      "1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.</s>\n",
      "\n",
      "<s>[INST]Explain photosynthesis in three sentences.[/INST]\n",
      "Response:\n",
      " 1. Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose (sugar), oxygen, and other chemical compounds.\n",
      "2. It is a vital process for the survival of most life forms on Earth, as it provides the primary source of energy and organic compounds necessary for growth and reproduction.\n",
      "3. Photosynthesis occurs in two stages: the light-dependent reactions, which take place in the thylakoid membranes of chloroplasts and involve the conversion of light energy into chemical energy in the form of ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate), and the light-independent reactions, which occur in the stroma of chloroplasts and involve the fixation of carbon dioxide into glucose through a series of chemical reactions known as the Calvin cycle."
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "fs_template = PromptTemplate.from_template(\n",
    "    \"\"\"<s>[INST] {question} [/INST]\n",
    "{output}</s>\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Explain precipitation in two sentences.\",\n",
    "        \"output\": \"1. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls from clouds due to gravitational pull.\\n2. The main forms of precipitation include drizzle, rain, sleet, snow, ice pellets, graupel and hail.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain condensation in one sentence.\",\n",
    "        \"output\": \"1. Condensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt_fs = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=fs_template,\n",
    "    suffix=\"<s>[INST]{question}[/INST]\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_fs.format(question=\"Explain photosynthesis in three sentences.\")\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"<|im_end|>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4fb9a-cb8a-4940-9715-4d70f4c215b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Using a couple examples, we were able to teach the model to respond by specifically numbering the sentences in its response and limiting its response to the number of sentences requested.</b></div>\n",
    "\n",
    "\n",
    "# Augmenting LLM response with context\n",
    "\n",
    "Without providing any real-time context, the model is limited to knowledge it gained during training which may be incomplete or out of date. However, we can improve responses by providing relevant context that the model can use before formulating a response. \n",
    "\n",
    "To illustrate the benefit of providing context we will first query the model about which instance types are available to use for managed spot training in SageMaker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f102be3d-1d7f-43c9-aa79-dbc2ccd21753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Which instances can I use with Managed Spot Training in SageMaker? [/INST]    \n",
      "\n",
      "Response:\n",
      "Managed Spot Training is a feature of Amazon SageMaker that allows you to train machine learning models using Amazon EC2 Spot Instances, which are instances that are available at a lower cost than On-Demand Instances. You can use Managed Spot Training in the following scenarios:\n",
      "\n",
      "1. Batch Inference: You can use Managed Spot Training for batch inference when you need to process large amounts of data and want to minimize costs.\n",
      "2. Model Training: You can use Managed Spot Training for model training when you have a long-running training job that can tolerate interruptions.\n",
      "3. Hyperparameter Tuning: You can use Managed Spot Training for hyperparameter tuning when you need to optimize the performance of your model and want to minimize costs.\n",
      "4. Model Serving: You can use Managed Spot Training for model serving when you have a large number of requests and want to minimize costs.\n",
      "5. Data Preprocessing: You can use Managed Spot Training for data preprocessing when you need to perform tasks such as data cleaning, feature engineering, and data augmentation.\n",
      "6. Model Deployment: You can use Managed Spot Training for model deployment when you have a large number of models and want to minimize costs."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(\n",
    "    question=\"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    ")\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b4409-3c5e-4c23-8a3b-39897bfee683",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>This response does not clearly and directly answer our question and you can see how without provided context, the LLM uses its own \"knowledge\" to answer questions. Let's try providing some context to the model to use as reference. We will add a new dynamic field \"context\" to our prompt template and provide up to date information when we ask the user question.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6110e2e4-773f-4ff6-a0d3-3d33073cae42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "Question: Which instances can I use with Managed Spot Training in SageMaker?[/INST]\n",
      "\n",
      "Response:\n",
      "You can use all instances supported in Amazon SageMaker with Managed Spot Training."
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "context_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}[/INST]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "context = \"\"\"Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\"\"\"\n",
    "\n",
    "\n",
    "prompt = context_template.format(\n",
    "    context=context,\n",
    "    question=\"Which instances can I use with Managed Spot Training in SageMaker?\",\n",
    ")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31be60b-e4e5-4e3c-98e2-a8ef42ceb889",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>Great! The model used the provided context to answer the user question. Now lets use the same context and ask the model about something that is not covered in the context. </b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f59ac84e-dd06-46c2-88ef-d5f0c9547f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "Question: Which instances can I use with Amazon ECS?[/INST]\n",
      "\n",
      "Response:\n",
      "I don't know, the context does not mention anything about using Managed Spot Training with Amazon ECS."
     ]
    }
   ],
   "source": [
    "prompt = context_template.format(\n",
    "    context=context, question=\"Which instances can I use with Amazon ECS?\"\n",
    ")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4324af-bfa3-4c47-a43b-ee6393fad0c5",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model followed our direction and admitted that it did not know the answer when it wasn't able to be determined from the provided context. This is a powerful method that is used heavily in RAG (Retrieval Augmented Generation) applications. A company may only want an IT support bot to provide answers based on internal documentaiton provided to the model as context. </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd469ce-4e37-4cec-8168-cadd5d072237",
   "metadata": {},
   "source": [
    "# Hallucinations\n",
    "\n",
    "Hallucination is when a model makes up information and presents it as if it were true. Leading questions can often cause hallucinations. Let's see what happens if we ask about a non-existent new launch at re:Invent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c3685e-03e7-4072-bc8c-8a67b63aaa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] What type of new chemical solution AWS announced at re:Invent this year? [/INST]    \n",
      "\n",
      "Response:\n",
      "At re:Invent 2021, AWS announced a new chemical solution called \"AWS Elemental AI for Chemical Processing.\" This solution is designed to help chemical processors optimize their operations and reduce costs by using machine learning algorithms to predict equipment failures, optimize reaction conditions, and improve product quality. The solution leverages AWS's existing expertise in machine learning and data analytics to provide a scalable and cost-effective solution for the chemical industry."
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(\n",
    "    question=\"What type of new chemical solution AWS announced at re:Invent this year?\"\n",
    ")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa8272-6572-47da-a365-b23ac32be304",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model incorrectly states that AWS announced a new chemical solution called \"AWS Graviton2\" in 2019 (you may get a different answer). One approach to combatting LLM hallucination is to give the model an \"out\" by giving it explicit instructions not to make up information and admit when it doesn't know something. Let's try the same question again by by also adding additional instructions in our prompt as follows: </b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b931cf35-e59e-491c-8772-0cda7f021590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] What type of new chemical solution AWS announced at re:Invent this year? Do not make up facts. Say I don't know if you have no information about something or unsure. [/INST]    \n",
      "\n",
      "Response:\n",
      "I apologize, but I do not have any specific information on the new chemical solution that AWS announced at re:Invent this year. Can you please provide me with more context or details so that I can assist you better?"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(\n",
    "    question=\"What type of new chemical solution AWS announced at re:Invent this year? Do not make up facts. Say I don't know if you have no information about something or unsure.\"\n",
    ")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f21e2-4b5f-4313-86e5-3885b2f10f1d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>By giving the model clear instructions and an out, we were able to get the model to admit it didn't have enough context to answer the question rather than making up something.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb206-6661-4ba6-95c5-6369176ff869",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "Let's make a simple chatbot.  There is no special library to include and no setting to apply to the LLM, all we need is prompt engineering!\n",
    "\n",
    "Let's use what we know of prompts with in context learning, to create a simple chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "898d619d-6c08-46d4-a0ba-cb1a9e23a7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Who is Jeff Bezos? [/INST]    \n",
      "\n",
      "Response:\n",
      "Jeff Bezos is an American entrepreneur and business magnate. He is the founder, CEO, CTO, and lead shareholder of Amazon, a multinational technology company focusing on e-commerce, cloud computing, digital streaming, and artificial intelligence. He is also the founder of Blue Origin, a space exploration company. Bezos has been ranked as the world's wealthiest person by Forbes multiple times.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Jeff Bezos?\"\n",
    "\n",
    "prompt = prompt_template.format(question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "resp = llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75da0c-cce2-4f04-8cc0-65ea5b1efc5b",
   "metadata": {},
   "source": [
    "### Follow-up questions\n",
    "Now if we ask the LLM a follow-up question without providing context from the conversation so far, it will not be able to provide an accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae5b02ab-f7d3-4022-b26d-85284e8fe9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] How old is he? [/INST]    \n",
      "\n",
      "Response:\n",
      "I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. Therefore, I don't know how old he is. Can you please provide more context or information about who you are referring to?"
     ]
    }
   ],
   "source": [
    "question = \"How old is he?\"\n",
    "\n",
    "prompt = prompt_template.format(question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422bf01-56ab-4d94-8395-bc8ffb592dd6",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>The model doesn't know who \"he\" is. Without the chat history context it is not able to answer.</b></div>\n",
    "\n",
    "### Providing chat context\n",
    "Let's use the context template we created earlier and feed in the previous response about Jeff Bezos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4500db7-a2da-40ce-b525-a33eacb46c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] Given the context below, answer the question that follows. If you do not know the answer and the context doesn't contain the answer truthfully say \"I don't know\".\n",
      "\n",
      "Context: Jeff Bezos is an American entrepreneur and business magnate. He is the founder, CEO, CTO, and lead shareholder of Amazon, a multinational technology company focusing on e-commerce, cloud computing, digital streaming, and artificial intelligence. He is also the founder of Blue Origin, a space exploration company. Bezos has been ranked as the world's wealthiest person by Forbes multiple times.\n",
      "Question: How old is he?[/INST]\n",
      "\n",
      "Response:\n",
      "I don't know, the context does not provide information about Jeff Bezos' age."
     ]
    }
   ],
   "source": [
    "prompt = context_template.format(context=resp, question=question)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(\"Response:\")\n",
    "for text in llm(prompt, max_new_tokens=300, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84aba05-9ae9-4511-a506-8bbf558854e9",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><b>This is a simplified example, but by including the past chat context, we can support a back and forth chat bot interaction with the LLM.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd190db-6d79-4a00-9d67-b0bfb4019a3f",
   "metadata": {},
   "source": [
    "# More examples of LLM use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7671b-87da-453b-a400-6f125d644b38",
   "metadata": {},
   "source": [
    "## The following are optional prompts you can try running to keep exploring.\n",
    "\n",
    "\n",
    "For simpler questions/prompts we can often get away witout strict instruction formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c507742b-dab9-4b97-b2c2-13f7cbfc1688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am considering applying for this position, however, I am uncertain about the process and would appreciate your assistance."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Rewrite the following sentence in better English:\n",
    "I think I want to apply for this position but don't know how, can you help?\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71e5cb14-1bbe-4ff3-a570-f2cb25742d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "This error occurs because the `zip` module is not installed on your system. The `zip` module provides functionality for working with compressed files in Python. You can install it using pip by running the following command in your terminal or command prompt:\n",
      "```\n",
      "pip install zipfile\n",
      "```\n",
      "Once you have installed the `zipfile` package, you should be able to run your code without encountering this error."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"I am getting the following error when attempting to run this pythong code, can you explain why?\n",
    "  File \"/home/studio-lab-user/sagemaker-studiolab-notebooks/ha.py\", line 2, in <module>\n",
    "    import zip\n",
    "ModuleNotFoundError: No module named 'zip'\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=150, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f84be33-49d8-463a-99ae-44b099b28da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduced Paragraph:\n",
      "### lives at ### ###, ### ### ###. His phone number is ### ### ### ### ###."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Reduct PII from the following paragraph. Replace any PII with ###.\n",
    "Paragraph:\n",
    "Jeff Bezos lives at 1 Main St. Miami, FL 39812. His phone number is 111-123-4567.\n",
    "\"\"\"\n",
    "\n",
    "for text in llm(prompt, max_new_tokens=250, temperature=0.1, stop=\"</s>\", stream=True):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a806d-7351-408c-b141-fee35b1853b7",
   "metadata": {},
   "source": [
    "---\n",
    "The last one or two examples didn't quite work. Perhaps the model is not strong enough for this type of task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac46a54-7a46-4e0d-b089-553106c1b25f",
   "metadata": {},
   "source": [
    "## Next steps: Want to use a different model with this notebook?\n",
    "\n",
    "Hugging Face have many models that you can use and drop in to code like this. We encourage you to play with the prompts above and other models. You may need to make modifications to the code above depending on the model you choose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405c238-40b5-4bf0-8d2d-271f7465080d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
